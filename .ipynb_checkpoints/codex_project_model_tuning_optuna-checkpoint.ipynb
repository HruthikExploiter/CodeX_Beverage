{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af375936-a4a4-4d88-9a4e-346370251860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3831932e-9b8a-41d9-aa8b-38942d86ff7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zone</th>\n",
       "      <th>occupation</th>\n",
       "      <th>income_levels</th>\n",
       "      <th>consume_frequency(weekly)</th>\n",
       "      <th>current_brand</th>\n",
       "      <th>preferable_consumption_size</th>\n",
       "      <th>awareness_of_other_brands</th>\n",
       "      <th>reasons_for_choosing_brands</th>\n",
       "      <th>flavor_preference</th>\n",
       "      <th>purchase_channel</th>\n",
       "      <th>packaging_preference</th>\n",
       "      <th>health_concerns</th>\n",
       "      <th>typical_consumption_situations</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R00001</td>\n",
       "      <td>30</td>\n",
       "      <td>M</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>&lt;10L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>Price</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>100-150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R00002</td>\n",
       "      <td>46</td>\n",
       "      <td>F</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>&gt; 35L</td>\n",
       "      <td>5-7 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Quality</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Social (eg. Parties)</td>\n",
       "      <td>200-250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R00003</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>&gt; 35L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Availability</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>200-250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R00004</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>5-7 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>Brand Reputation</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Eco-Friendly</td>\n",
       "      <td>Low (Not very concerned)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R00005</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Student</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>Availability</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>50-100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent_id  age gender   zone            occupation income_levels  \\\n",
       "0        R00001   30      M  Urban  Working Professional          <10L   \n",
       "1        R00002   46      F  Metro  Working Professional         > 35L   \n",
       "2        R00003   41      F  Rural  Working Professional         > 35L   \n",
       "3        R00004   33      F  Urban  Working Professional     16L - 25L   \n",
       "4        R00005   23      M  Metro               Student           NaN   \n",
       "\n",
       "  consume_frequency(weekly) current_brand preferable_consumption_size  \\\n",
       "0                 3-4 times      Newcomer             Medium (500 ml)   \n",
       "1                 5-7 times   Established             Medium (500 ml)   \n",
       "2                 3-4 times      Newcomer             Medium (500 ml)   \n",
       "3                 5-7 times      Newcomer             Medium (500 ml)   \n",
       "4                 3-4 times   Established             Medium (500 ml)   \n",
       "\n",
       "  awareness_of_other_brands reasons_for_choosing_brands flavor_preference  \\\n",
       "0                    0 to 1                       Price       Traditional   \n",
       "1                    2 to 4                     Quality            Exotic   \n",
       "2                    2 to 4                Availability       Traditional   \n",
       "3                    0 to 1            Brand Reputation            Exotic   \n",
       "4                    0 to 1                Availability       Traditional   \n",
       "\n",
       "  purchase_channel packaging_preference                       health_concerns  \\\n",
       "0           Online               Simple  Medium (Moderately health-conscious)   \n",
       "1     Retail Store              Premium  Medium (Moderately health-conscious)   \n",
       "2     Retail Store              Premium  Medium (Moderately health-conscious)   \n",
       "3           Online         Eco-Friendly              Low (Not very concerned)   \n",
       "4           Online              Premium  Medium (Moderately health-conscious)   \n",
       "\n",
       "  typical_consumption_situations price_range  \n",
       "0       Active (eg. Sports, gym)     100-150  \n",
       "1           Social (eg. Parties)     200-250  \n",
       "2       Active (eg. Sports, gym)     200-250  \n",
       "3       Active (eg. Sports, gym)     150-200  \n",
       "4       Active (eg. Sports, gym)      50-100  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\hruth\\Desktop\\AI engineer\\Virtual Internship\\week 3&4\\datasets\\survey_results.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdeba276-6969-47dd-9b98-ae85a06d0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30010 entries, 0 to 30009\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   respondent_id                   30010 non-null  object\n",
      " 1   age                             30010 non-null  int64 \n",
      " 2   gender                          30010 non-null  object\n",
      " 3   zone                            30010 non-null  object\n",
      " 4   occupation                      30010 non-null  object\n",
      " 5   income_levels                   21946 non-null  object\n",
      " 6   consume_frequency(weekly)       30002 non-null  object\n",
      " 7   current_brand                   30010 non-null  object\n",
      " 8   preferable_consumption_size     30010 non-null  object\n",
      " 9   awareness_of_other_brands       30010 non-null  object\n",
      " 10  reasons_for_choosing_brands     30010 non-null  object\n",
      " 11  flavor_preference               30010 non-null  object\n",
      " 12  purchase_channel                30000 non-null  object\n",
      " 13  packaging_preference            30010 non-null  object\n",
      " 14  health_concerns                 30010 non-null  object\n",
      " 15  typical_consumption_situations  30010 non-null  object\n",
      " 16  price_range                     30010 non-null  object\n",
      "dtypes: int64(1), object(16)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926e9959-181f-44ea-bc96-c9de5d59ba65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30010.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.047684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.439250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>604.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age\n",
       "count  30010.000000\n",
       "mean      33.047684\n",
       "std       13.439250\n",
       "min       18.000000\n",
       "25%       23.000000\n",
       "50%       31.000000\n",
       "75%       40.000000\n",
       "max      604.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c16c2de-9ff8-445a-b5eb-8307751627f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0932c57-6b2f-4a1f-bf4f-528cdbec0dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zone</th>\n",
       "      <th>occupation</th>\n",
       "      <th>income_levels</th>\n",
       "      <th>consume_frequency(weekly)</th>\n",
       "      <th>current_brand</th>\n",
       "      <th>preferable_consumption_size</th>\n",
       "      <th>awareness_of_other_brands</th>\n",
       "      <th>reasons_for_choosing_brands</th>\n",
       "      <th>flavor_preference</th>\n",
       "      <th>purchase_channel</th>\n",
       "      <th>packaging_preference</th>\n",
       "      <th>health_concerns</th>\n",
       "      <th>typical_consumption_situations</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>R02309</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Brand Reputation</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Social (eg. Parties)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>R02665</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>above 4</td>\n",
       "      <td>Brand Reputation</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Simple</td>\n",
       "      <td>High (Very health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>200-250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>R05149</td>\n",
       "      <td>21</td>\n",
       "      <td>M</td>\n",
       "      <td>Semi-Urban</td>\n",
       "      <td>Student</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Availability</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Low (Not very concerned)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>50-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7794</th>\n",
       "      <td>R07791</td>\n",
       "      <td>25</td>\n",
       "      <td>M</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>&lt;10L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Availability</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Casual (eg. At home)</td>\n",
       "      <td>100-150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8516</th>\n",
       "      <td>R08512</td>\n",
       "      <td>20</td>\n",
       "      <td>F</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Student</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5-7 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Large (1 L)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>Price</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Casual (eg. At home)</td>\n",
       "      <td>100-150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11317</th>\n",
       "      <td>R11312</td>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Student</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-2 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Availability</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Social (eg. Parties)</td>\n",
       "      <td>100-150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19050</th>\n",
       "      <td>R19044</td>\n",
       "      <td>57</td>\n",
       "      <td>M</td>\n",
       "      <td>Semi-Urban</td>\n",
       "      <td>Retired</td>\n",
       "      <td>&lt;10L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Large (1 L)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Price</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Premium</td>\n",
       "      <td>High (Very health-conscious)</td>\n",
       "      <td>Social (eg. Parties)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22412</th>\n",
       "      <td>R22405</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>0-2 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>Price</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Low (Not very concerned)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23406</th>\n",
       "      <td>R23398</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>10L - 15L</td>\n",
       "      <td>5-7 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Large (1 L)</td>\n",
       "      <td>above 4</td>\n",
       "      <td>Availability</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Premium</td>\n",
       "      <td>High (Very health-conscious)</td>\n",
       "      <td>Casual (eg. At home)</td>\n",
       "      <td>200-250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25068</th>\n",
       "      <td>R25059</td>\n",
       "      <td>34</td>\n",
       "      <td>F</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>0-2 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Brand Reputation</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Eco-Friendly</td>\n",
       "      <td>High (Very health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      respondent_id  age gender        zone            occupation  \\\n",
       "2309         R02309   27      M       Urban  Working Professional   \n",
       "2666         R02665   61      M       Metro          Entrepreneur   \n",
       "5151         R05149   21      M  Semi-Urban               Student   \n",
       "7794         R07791   25      M       Metro  Working Professional   \n",
       "8516         R08512   20      F       Metro               Student   \n",
       "11317        R11312   18      F       Urban               Student   \n",
       "19050        R19044   57      M  Semi-Urban               Retired   \n",
       "22412        R22405   26      M       Metro  Working Professional   \n",
       "23406        R23398   27      M       Urban  Working Professional   \n",
       "25068        R25059   34      F       Urban  Working Professional   \n",
       "\n",
       "      income_levels consume_frequency(weekly) current_brand  \\\n",
       "2309      16L - 25L                 3-4 times      Newcomer   \n",
       "2666      16L - 25L                 3-4 times   Established   \n",
       "5151            NaN                 3-4 times   Established   \n",
       "7794           <10L                 3-4 times      Newcomer   \n",
       "8516            NaN                 5-7 times   Established   \n",
       "11317           NaN                 0-2 times      Newcomer   \n",
       "19050          <10L                 3-4 times      Newcomer   \n",
       "22412     16L - 25L                 0-2 times      Newcomer   \n",
       "23406     10L - 15L                 5-7 times      Newcomer   \n",
       "25068     16L - 25L                 0-2 times      Newcomer   \n",
       "\n",
       "      preferable_consumption_size awareness_of_other_brands  \\\n",
       "2309               Small (250 ml)                    2 to 4   \n",
       "2666              Medium (500 ml)                   above 4   \n",
       "5151               Small (250 ml)                    2 to 4   \n",
       "7794              Medium (500 ml)                    2 to 4   \n",
       "8516                  Large (1 L)                    0 to 1   \n",
       "11317             Medium (500 ml)                    2 to 4   \n",
       "19050                 Large (1 L)                    2 to 4   \n",
       "22412              Small (250 ml)                    0 to 1   \n",
       "23406                 Large (1 L)                   above 4   \n",
       "25068              Small (250 ml)                    2 to 4   \n",
       "\n",
       "      reasons_for_choosing_brands flavor_preference purchase_channel  \\\n",
       "2309             Brand Reputation       Traditional           Online   \n",
       "2666             Brand Reputation            Exotic     Retail Store   \n",
       "5151                 Availability       Traditional     Retail Store   \n",
       "7794                 Availability            Exotic           Online   \n",
       "8516                        Price            Exotic           Online   \n",
       "11317                Availability            Exotic           Online   \n",
       "19050                       Price       Traditional     Retail Store   \n",
       "22412                       Price            Exotic           Online   \n",
       "23406                Availability       Traditional           Online   \n",
       "25068            Brand Reputation            Exotic           Online   \n",
       "\n",
       "      packaging_preference                       health_concerns  \\\n",
       "2309                Simple  Medium (Moderately health-conscious)   \n",
       "2666                Simple          High (Very health-conscious)   \n",
       "5151                Simple              Low (Not very concerned)   \n",
       "7794                Simple  Medium (Moderately health-conscious)   \n",
       "8516                Simple  Medium (Moderately health-conscious)   \n",
       "11317              Premium  Medium (Moderately health-conscious)   \n",
       "19050              Premium          High (Very health-conscious)   \n",
       "22412              Premium              Low (Not very concerned)   \n",
       "23406              Premium          High (Very health-conscious)   \n",
       "25068         Eco-Friendly          High (Very health-conscious)   \n",
       "\n",
       "      typical_consumption_situations price_range  \n",
       "2309            Social (eg. Parties)     150-200  \n",
       "2666        Active (eg. Sports, gym)     200-250  \n",
       "5151        Active (eg. Sports, gym)      50-100  \n",
       "7794            Casual (eg. At home)     100-150  \n",
       "8516            Casual (eg. At home)     100-150  \n",
       "11317           Social (eg. Parties)     100-150  \n",
       "19050           Social (eg. Parties)     150-200  \n",
       "22412       Active (eg. Sports, gym)     150-200  \n",
       "23406           Casual (eg. At home)     200-250  \n",
       "25068       Active (eg. Sports, gym)     150-200  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "207339bf-91de-406e-bc8a-aa50b1a6cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3b775b-6b12-434f-b5d6-76aefeff9aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59900cc3-5fe5-4200-87c6-d326d6af92eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot(df['age'], vert=True)\n",
    "plt.title(\"Box Plot of Age\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d741dba8-b8c9-4864-9248-c40b9e8d59b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zone</th>\n",
       "      <th>occupation</th>\n",
       "      <th>income_levels</th>\n",
       "      <th>consume_frequency(weekly)</th>\n",
       "      <th>current_brand</th>\n",
       "      <th>preferable_consumption_size</th>\n",
       "      <th>awareness_of_other_brands</th>\n",
       "      <th>reasons_for_choosing_brands</th>\n",
       "      <th>flavor_preference</th>\n",
       "      <th>purchase_channel</th>\n",
       "      <th>packaging_preference</th>\n",
       "      <th>health_concerns</th>\n",
       "      <th>typical_consumption_situations</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>R03098</td>\n",
       "      <td>453</td>\n",
       "      <td>M</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>26L - 35L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Brand Reputation</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Social (eg. Parties)</td>\n",
       "      <td>200-250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6259</th>\n",
       "      <td>R06260</td>\n",
       "      <td>428</td>\n",
       "      <td>M</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Entrepreneur</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>5-7 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Large (1 L)</td>\n",
       "      <td>above 4</td>\n",
       "      <td>Quality</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>High (Very health-conscious)</td>\n",
       "      <td>Social (eg. Parties)</td>\n",
       "      <td>200-250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12397</th>\n",
       "      <td>R12398</td>\n",
       "      <td>604</td>\n",
       "      <td>M</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Retired</td>\n",
       "      <td>&lt;10L</td>\n",
       "      <td>0-2 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Availability</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Casual (eg. At home)</td>\n",
       "      <td>100-150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22541</th>\n",
       "      <td>R22542</td>\n",
       "      <td>457</td>\n",
       "      <td>M</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Price</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Premium</td>\n",
       "      <td>High (Very health-conscious)</td>\n",
       "      <td>Casual (eg. At home)</td>\n",
       "      <td>200-250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22910</th>\n",
       "      <td>R22911</td>\n",
       "      <td>267</td>\n",
       "      <td>F</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>above 4</td>\n",
       "      <td>Brand Reputation</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Eco-Friendly</td>\n",
       "      <td>Low (Not very concerned)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24949</th>\n",
       "      <td>R24950</td>\n",
       "      <td>285</td>\n",
       "      <td>M</td>\n",
       "      <td>Semi-Urban</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>&gt; 35L</td>\n",
       "      <td>0-2 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Price</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24951</th>\n",
       "      <td>R24952</td>\n",
       "      <td>192</td>\n",
       "      <td>F</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Student</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>Newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>Price</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Low (Not very concerned)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>50-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25095</th>\n",
       "      <td>R25096</td>\n",
       "      <td>203</td>\n",
       "      <td>M</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>16L - 25L</td>\n",
       "      <td>0-2 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Brand Reputation</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Premium</td>\n",
       "      <td>High (Very health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28760</th>\n",
       "      <td>R28761</td>\n",
       "      <td>428</td>\n",
       "      <td>F</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Working Professional</td>\n",
       "      <td>26L - 35L</td>\n",
       "      <td>0-2 times</td>\n",
       "      <td>Established</td>\n",
       "      <td>Small (250 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>Brand Reputation</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Casual (eg. At home)</td>\n",
       "      <td>150-200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      respondent_id  age gender        zone            occupation  \\\n",
       "3097         R03098  453      M       Metro  Working Professional   \n",
       "6259         R06260  428      M       Urban          Entrepreneur   \n",
       "12397        R12398  604      M       Metro               Retired   \n",
       "22541        R22542  457      M       Metro  Working Professional   \n",
       "22910        R22911  267      F       Metro  Working Professional   \n",
       "24949        R24950  285      M  Semi-Urban  Working Professional   \n",
       "24951        R24952  192      F       Urban               Student   \n",
       "25095        R25096  203      M       Metro  Working Professional   \n",
       "28760        R28761  428      F       Rural  Working Professional   \n",
       "\n",
       "      income_levels consume_frequency(weekly) current_brand  \\\n",
       "3097      26L - 35L                 3-4 times   Established   \n",
       "6259      16L - 25L                 5-7 times   Established   \n",
       "12397          <10L                 0-2 times      Newcomer   \n",
       "22541     16L - 25L                 3-4 times      Newcomer   \n",
       "22910     16L - 25L                 3-4 times   Established   \n",
       "24949         > 35L                 0-2 times      Newcomer   \n",
       "24951           NaN                 3-4 times      Newcomer   \n",
       "25095     16L - 25L                 0-2 times   Established   \n",
       "28760     26L - 35L                 0-2 times   Established   \n",
       "\n",
       "      preferable_consumption_size awareness_of_other_brands  \\\n",
       "3097              Medium (500 ml)                    2 to 4   \n",
       "6259                  Large (1 L)                   above 4   \n",
       "12397              Small (250 ml)                    2 to 4   \n",
       "22541              Small (250 ml)                    2 to 4   \n",
       "22910             Medium (500 ml)                   above 4   \n",
       "24949              Small (250 ml)                    2 to 4   \n",
       "24951             Medium (500 ml)                    0 to 1   \n",
       "25095              Small (250 ml)                    2 to 4   \n",
       "28760              Small (250 ml)                    2 to 4   \n",
       "\n",
       "      reasons_for_choosing_brands flavor_preference purchase_channel  \\\n",
       "3097             Brand Reputation       Traditional     Retail Store   \n",
       "6259                      Quality            Exotic           Online   \n",
       "12397                Availability       Traditional           Online   \n",
       "22541                       Price       Traditional           Online   \n",
       "22910            Brand Reputation            Exotic           Online   \n",
       "24949                       Price       Traditional           Online   \n",
       "24951                       Price       Traditional           Online   \n",
       "25095            Brand Reputation            Exotic     Retail Store   \n",
       "28760            Brand Reputation            Exotic           Online   \n",
       "\n",
       "      packaging_preference                       health_concerns  \\\n",
       "3097               Premium  Medium (Moderately health-conscious)   \n",
       "6259                Simple          High (Very health-conscious)   \n",
       "12397               Simple  Medium (Moderately health-conscious)   \n",
       "22541              Premium          High (Very health-conscious)   \n",
       "22910         Eco-Friendly              Low (Not very concerned)   \n",
       "24949               Simple  Medium (Moderately health-conscious)   \n",
       "24951               Simple              Low (Not very concerned)   \n",
       "25095              Premium          High (Very health-conscious)   \n",
       "28760               Simple  Medium (Moderately health-conscious)   \n",
       "\n",
       "      typical_consumption_situations price_range  \n",
       "3097            Social (eg. Parties)     200-250  \n",
       "6259            Social (eg. Parties)     200-250  \n",
       "12397           Casual (eg. At home)     100-150  \n",
       "22541           Casual (eg. At home)     200-250  \n",
       "22910       Active (eg. Sports, gym)     150-200  \n",
       "24949       Active (eg. Sports, gym)     150-200  \n",
       "24951       Active (eg. Sports, gym)      50-100  \n",
       "25095       Active (eg. Sports, gym)     150-200  \n",
       "28760           Casual (eg. At home)     150-200  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_ages = df[df['age'] > 100]\n",
    "invalid_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f50ccad-7042-4a5e-9da4-4b3c9f7ac4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['occupation'] = df['occupation'].astype(str).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d7fd08-aa2b-4089-a66a-f95eb2b1dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after valid age filtering: 29991\n"
     ]
    }
   ],
   "source": [
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "df = df[(df['age'] >= 18) & (df['age'] <= 70)].reset_index(drop=True)\n",
    "print(\"Rows after valid age filtering:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b039cad-3962-4802-b22d-c0d5054d6905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    29991.000000\n",
       "mean        32.947484\n",
       "std         11.906077\n",
       "min         18.000000\n",
       "25%         23.000000\n",
       "50%         31.000000\n",
       "75%         40.000000\n",
       "max         70.000000\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ffa0b7-8af5-4051-898f-4b148cb42f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zone</th>\n",
       "      <th>occupation</th>\n",
       "      <th>income_levels</th>\n",
       "      <th>consume_frequency(weekly)</th>\n",
       "      <th>current_brand</th>\n",
       "      <th>preferable_consumption_size</th>\n",
       "      <th>awareness_of_other_brands</th>\n",
       "      <th>reasons_for_choosing_brands</th>\n",
       "      <th>flavor_preference</th>\n",
       "      <th>purchase_channel</th>\n",
       "      <th>packaging_preference</th>\n",
       "      <th>health_concerns</th>\n",
       "      <th>typical_consumption_situations</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [respondent_id, age, gender, zone, occupation, income_levels, consume_frequency(weekly), current_brand, preferable_consumption_size, awareness_of_other_brands, reasons_for_choosing_brands, flavor_preference, purchase_channel, packaging_preference, health_concerns, typical_consumption_situations, price_range]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"age\"]>70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51520c74-7b1d-4cb7-bfcb-1c6980a0a402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3-4 times'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['consume_frequency(weekly)'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72c25ef0-50f0-41d7-a2c3-c1eec5debf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zone\n",
       "Metro         11906\n",
       "Urban         10686\n",
       "Semi-Urban     5274\n",
       "Rural          2116\n",
       "urbna             5\n",
       "Metor             4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['zone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899b48fb-a883-4368-b0fa-735feb412c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handiling the missing data\n",
    "\n",
    "df['income_levels'] = df['income_levels'].fillna('Not Reported')\n",
    "\n",
    "df['consume_frequency(weekly)'] = df['consume_frequency(weekly)'].fillna(\n",
    "    df['consume_frequency(weekly)'].mode()[0]\n",
    ")\n",
    "\n",
    "df['purchase_channel'] = df['purchase_channel'].fillna(\n",
    "    df['purchase_channel'].mode()[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32f3378a-3b68-4c69-8b97-816c9af4adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling mistakes in caegorical data\n",
    "\n",
    "df['zone'] = df['zone'].str.lower().str.strip()\n",
    "df['current_brand'] = df['current_brand'].str.lower().str.strip()\n",
    "\n",
    "df[\"zone\"] = df[\"zone\"].replace({\n",
    "    \"metor\": \"metro\",\n",
    "    \"urbna\": \"urban\"\n",
    "})\n",
    "\n",
    "df['current_brand'] = df['current_brand'].replace({\n",
    "    'establishd': 'established'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb253bf-7c51-4b37-bc60-d30352fde96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_group'] = pd.cut(\n",
    "    df['age'],\n",
    "    bins=[17,25,35,45,55,70,float('inf')],\n",
    "    labels=['18-25','26-35','36-45','46-55','56-70','70+']\n",
    ")\n",
    "\n",
    "# Drop age column\n",
    "df = df.drop(columns=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ca8bc70-853c-4434-80fb-ade507e4ff01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26-35\n",
       "1    46-55\n",
       "2    36-45\n",
       "3    26-35\n",
       "4    18-25\n",
       "Name: age_group, dtype: category\n",
       "Categories (6, object): ['18-25' < '26-35' < '36-45' < '46-55' < '56-70' < '70+']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"age_group\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2b65ed0-2110-4a43-86c5-385c93a9de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting cf_ab_score\n",
    "\n",
    "consume_frequency_map = {\n",
    "    \"0-2 times\" : 1,\n",
    "    \"3-4 times\" : 2,\n",
    "    \"5-7 times\" : 3\n",
    "}\n",
    "\n",
    "awerness_of_other_brands_map = {\n",
    "    \"0 to 1\" : 1,\n",
    "    \"2 to 4\" : 2,\n",
    "    \"above 4\" : 3\n",
    "}\n",
    "\n",
    "\n",
    "df[\"frequency_score\"] = df[\"consume_frequency(weekly)\"].map(consume_frequency_map)\n",
    "df[\"awareness_score\"] = df[\"awareness_of_other_brands\"].map(awerness_of_other_brands_map)\n",
    "\n",
    "df[\"cf_ab_score\"] = df[\"frequency_score\"]/(df[\"awareness_score\"] + df[\"frequency_score\"])\n",
    "df[\"cf_ab_score\"] = df[\"cf_ab_score\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a70e570a-ff31-4dfc-9cd8-488ce3593525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['urban', 'metro', 'rural', 'semi-urban'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['zone'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2704f2cd-df45-40a7-b891-d296b6e760c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<10L', '> 35L', '16L - 25L', 'Not Reported', '10L - 15L',\n",
       "       '26L - 35L'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['income_levels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32edf22e-3f9d-4407-accc-53e2b9fa595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting zas_score\n",
    "df['income_levels'] = df['income_levels'].str.lower().str.strip()\n",
    "\n",
    "zone_map = {\n",
    "    \"rural\" : 1,\n",
    "    \"semi-urban\" : 2,\n",
    "    \"urban\" : 3,\n",
    "    \"metro\" : 4\n",
    "}\n",
    "\n",
    "income_map = {\n",
    "    \"not reported\": 0,\n",
    "    \"<10l\": 1,\n",
    "    \"10l - 15l\": 2,\n",
    "    \"16l - 25l\": 3,\n",
    "    \"26l - 35l\": 4,\n",
    "    \"> 35l\": 5\n",
    "}\n",
    "\n",
    "df[\"zone_score\"] = df[\"zone\"].map(zone_map)\n",
    "df[\"income_score\"] = df[\"income_levels\"].map(income_map)\n",
    "\n",
    "df[\"zas_score\"] = df[\"zone_score\"] * df[\"income_score\"]\n",
    "df[\"zas_score\"] = df[\"zas_score\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f2d2f70-348e-42e6-87f4-858a1c4a7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['zone_score', 'income_score'], errors = \"ignore\")\n",
    "df = df.drop(columns=['frequency_score', 'awareness_score'], errors ='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49e50fce-da6b-4898-b2ae-713f3ec359c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['newcomer', 'established'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['current_brand'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c7cd112-0836-40a6-a1bf-1a932c24991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reasons_for_choosing_brands'] = df['reasons_for_choosing_brands'].str.lower().str.strip()\n",
    "\n",
    "df['bsi'] = (\n",
    "    (df['current_brand'] != 'established') &\n",
    "    (df['reasons_for_choosing_brands'].isin(['price', 'quality']))\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b6145a9-d49b-46a8-b26e-51eb88cba7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>zone</th>\n",
       "      <th>occupation</th>\n",
       "      <th>income_levels</th>\n",
       "      <th>consume_frequency(weekly)</th>\n",
       "      <th>current_brand</th>\n",
       "      <th>preferable_consumption_size</th>\n",
       "      <th>awareness_of_other_brands</th>\n",
       "      <th>reasons_for_choosing_brands</th>\n",
       "      <th>flavor_preference</th>\n",
       "      <th>purchase_channel</th>\n",
       "      <th>packaging_preference</th>\n",
       "      <th>health_concerns</th>\n",
       "      <th>typical_consumption_situations</th>\n",
       "      <th>price_range</th>\n",
       "      <th>age_group</th>\n",
       "      <th>cf_ab_score</th>\n",
       "      <th>zas_score</th>\n",
       "      <th>bsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R00001</td>\n",
       "      <td>M</td>\n",
       "      <td>urban</td>\n",
       "      <td>working professional</td>\n",
       "      <td>&lt;10l</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>price</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Simple</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>100-150</td>\n",
       "      <td>26-35</td>\n",
       "      <td>0.67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R00002</td>\n",
       "      <td>F</td>\n",
       "      <td>metro</td>\n",
       "      <td>working professional</td>\n",
       "      <td>&gt; 35l</td>\n",
       "      <td>5-7 times</td>\n",
       "      <td>established</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>quality</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Social (eg. Parties)</td>\n",
       "      <td>200-250</td>\n",
       "      <td>46-55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R00003</td>\n",
       "      <td>F</td>\n",
       "      <td>rural</td>\n",
       "      <td>working professional</td>\n",
       "      <td>&gt; 35l</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>2 to 4</td>\n",
       "      <td>availability</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Retail Store</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>200-250</td>\n",
       "      <td>36-45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R00004</td>\n",
       "      <td>F</td>\n",
       "      <td>urban</td>\n",
       "      <td>working professional</td>\n",
       "      <td>16l - 25l</td>\n",
       "      <td>5-7 times</td>\n",
       "      <td>newcomer</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>brand reputation</td>\n",
       "      <td>Exotic</td>\n",
       "      <td>Online</td>\n",
       "      <td>Eco-Friendly</td>\n",
       "      <td>Low (Not very concerned)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>150-200</td>\n",
       "      <td>26-35</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R00005</td>\n",
       "      <td>M</td>\n",
       "      <td>metro</td>\n",
       "      <td>student</td>\n",
       "      <td>not reported</td>\n",
       "      <td>3-4 times</td>\n",
       "      <td>established</td>\n",
       "      <td>Medium (500 ml)</td>\n",
       "      <td>0 to 1</td>\n",
       "      <td>availability</td>\n",
       "      <td>Traditional</td>\n",
       "      <td>Online</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Medium (Moderately health-conscious)</td>\n",
       "      <td>Active (eg. Sports, gym)</td>\n",
       "      <td>50-100</td>\n",
       "      <td>18-25</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  respondent_id gender   zone            occupation income_levels  \\\n",
       "0        R00001      M  urban  working professional          <10l   \n",
       "1        R00002      F  metro  working professional         > 35l   \n",
       "2        R00003      F  rural  working professional         > 35l   \n",
       "3        R00004      F  urban  working professional     16l - 25l   \n",
       "4        R00005      M  metro               student  not reported   \n",
       "\n",
       "  consume_frequency(weekly) current_brand preferable_consumption_size  \\\n",
       "0                 3-4 times      newcomer             Medium (500 ml)   \n",
       "1                 5-7 times   established             Medium (500 ml)   \n",
       "2                 3-4 times      newcomer             Medium (500 ml)   \n",
       "3                 5-7 times      newcomer             Medium (500 ml)   \n",
       "4                 3-4 times   established             Medium (500 ml)   \n",
       "\n",
       "  awareness_of_other_brands reasons_for_choosing_brands flavor_preference  \\\n",
       "0                    0 to 1                       price       Traditional   \n",
       "1                    2 to 4                     quality            Exotic   \n",
       "2                    2 to 4                availability       Traditional   \n",
       "3                    0 to 1            brand reputation            Exotic   \n",
       "4                    0 to 1                availability       Traditional   \n",
       "\n",
       "  purchase_channel packaging_preference                       health_concerns  \\\n",
       "0           Online               Simple  Medium (Moderately health-conscious)   \n",
       "1     Retail Store              Premium  Medium (Moderately health-conscious)   \n",
       "2     Retail Store              Premium  Medium (Moderately health-conscious)   \n",
       "3           Online         Eco-Friendly              Low (Not very concerned)   \n",
       "4           Online              Premium  Medium (Moderately health-conscious)   \n",
       "\n",
       "  typical_consumption_situations price_range age_group  cf_ab_score  \\\n",
       "0       Active (eg. Sports, gym)     100-150     26-35         0.67   \n",
       "1           Social (eg. Parties)     200-250     46-55         0.60   \n",
       "2       Active (eg. Sports, gym)     200-250     36-45         0.50   \n",
       "3       Active (eg. Sports, gym)     150-200     26-35         0.75   \n",
       "4       Active (eg. Sports, gym)      50-100     18-25         0.67   \n",
       "\n",
       "   zas_score  bsi  \n",
       "0          3    1  \n",
       "1         20    0  \n",
       "2          5    0  \n",
       "3          9    0  \n",
       "4          0    0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8200a38-ac02-4978-8135-ec1f7654eaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['respondent_id', 'gender', 'zone', 'occupation', 'income_levels',\n",
       "       'consume_frequency(weekly)', 'current_brand',\n",
       "       'preferable_consumption_size', 'awareness_of_other_brands',\n",
       "       'reasons_for_choosing_brands', 'flavor_preference', 'purchase_channel',\n",
       "       'packaging_preference', 'health_concerns',\n",
       "       'typical_consumption_situations', 'price_range', 'age_group',\n",
       "       'cf_ab_score', 'zas_score', 'bsi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecf35e78-3752-489c-a793-63e2029ef451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.75)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cf_ab_score\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebd43626-f6a1-4cde-b1d1-8746676558aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 20,  5,  9,  0,  6, 12,  2,  4,  8, 15, 16,  1, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"zas_score\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ba24978-fe13-415f-a46d-56856fe7551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 0 0\n"
     ]
    }
   ],
   "source": [
    "bad1 = df[(df['age_group'] == '56-70') & (df['occupation'] == 'student')]\n",
    "bad2 = df[(df['age_group'] == '18-25') & (df['occupation'] == 'retired')]\n",
    "bad3 = df[(df['age_group'] == '18-25') & (df['occupation'] == 'homemaker')]\n",
    "\n",
    "print(len(bad1), len(bad2), len(bad3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6539f1e8-7dcc-4a7f-933c-40ccf12bd58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(bad1.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fe6aaea-0f7e-429a-93cc-d09ca81b6d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29956, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6c255b9-1fa4-49fc-94af-79ab4d25a82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20796"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['bsi'] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca996574-3ccc-4ebd-99fd-c3c31cb32e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['respondent_id', 'gender', 'zone', 'occupation', 'income_levels',\n",
       "       'consume_frequency(weekly)', 'current_brand',\n",
       "       'preferable_consumption_size', 'awareness_of_other_brands',\n",
       "       'reasons_for_choosing_brands', 'flavor_preference', 'purchase_channel',\n",
       "       'packaging_preference', 'health_concerns',\n",
       "       'typical_consumption_situations', 'price_range', 'age_group',\n",
       "       'cf_ab_score', 'zas_score', 'bsi'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38a9a000-134d-414d-8c2c-c4facc34d09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "respondent_id                       object\n",
       "gender                              object\n",
       "zone                                object\n",
       "occupation                          object\n",
       "income_levels                       object\n",
       "consume_frequency(weekly)           object\n",
       "current_brand                       object\n",
       "preferable_consumption_size         object\n",
       "awareness_of_other_brands           object\n",
       "reasons_for_choosing_brands         object\n",
       "flavor_preference                   object\n",
       "purchase_channel                    object\n",
       "packaging_preference                object\n",
       "health_concerns                     object\n",
       "typical_consumption_situations      object\n",
       "price_range                         object\n",
       "age_group                         category\n",
       "cf_ab_score                        float64\n",
       "zas_score                            int64\n",
       "bsi                                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6395b70d-b228-4f53-8f49-664972a862ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into independent and dependent variables \n",
    "\n",
    "X = df.drop(columns = [\"respondent_id\", \"price_range\"])\n",
    "y = df[\"price_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcfb6555-a6d7-4080-9999-5e6d9fb2dfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29956, 18)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c3e1e17-23e3-4b50-b2c3-0914402efa65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29956,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4223d770-cb42-4220-9258-18ade8d0b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90a6f960-6248-4040-87af-a90a2e7b4a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22467, 18), (7489, 18))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "407d0e7d-9c5b-4660-8f9c-8b75f800674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22467,), (7489,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "749ff108-83f3-46f1-a289-eca06eed5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "label_cols = [\n",
    "    'age_group',\n",
    "    'income_levels',\n",
    "    'health_concerns',\n",
    "    'consume_frequency(weekly)',\n",
    "    'preferable_consumption_size'\n",
    "]\n",
    "\n",
    "for col in label_cols:\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    X_test[col]  = le.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "905fe669-2bff-4022-b877-277318bf65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81e8547e-c615-4ee5-9f85-9ff792cba7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "onehot_cols = [\n",
    "    'gender',\n",
    "    'zone',\n",
    "    'occupation',\n",
    "    'current_brand',\n",
    "    'awareness_of_other_brands',\n",
    "    'reasons_for_choosing_brands',\n",
    "    'flavor_preference',\n",
    "    'purchase_channel',\n",
    "    'packaging_preference',\n",
    "    'typical_consumption_situations'\n",
    "]\n",
    "\n",
    "for col in onehot_cols:\n",
    "    \n",
    "    ohe.fit(X_train[[col]])\n",
    "\n",
    "    # transform\n",
    "    train_ohe = ohe.transform(X_train[[col]])\n",
    "    test_ohe = ohe.transform(X_test[[col]])\n",
    "\n",
    "    # assign\n",
    "    X_train[ohe.get_feature_names_out([col])] = train_ohe.astype(int)\n",
    "    X_test[ohe.get_feature_names_out([col])] = test_ohe.astype(int)\n",
    "\n",
    "    # drop original\n",
    "    X_train.drop(columns=[col], inplace=True)\n",
    "    X_test.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "756702fe-0da4-4123-9d2a-008cd72256e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22467, 27), (7489, 27))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc24ddd6-4efd-4b92-b55a-100ec1c88332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_levels</th>\n",
       "      <th>consume_frequency(weekly)</th>\n",
       "      <th>preferable_consumption_size</th>\n",
       "      <th>health_concerns</th>\n",
       "      <th>age_group</th>\n",
       "      <th>cf_ab_score</th>\n",
       "      <th>zas_score</th>\n",
       "      <th>bsi</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>zone_rural</th>\n",
       "      <th>...</th>\n",
       "      <th>awareness_of_other_brands_above 4</th>\n",
       "      <th>reasons_for_choosing_brands_brand reputation</th>\n",
       "      <th>reasons_for_choosing_brands_price</th>\n",
       "      <th>reasons_for_choosing_brands_quality</th>\n",
       "      <th>flavor_preference_Traditional</th>\n",
       "      <th>purchase_channel_Retail Store</th>\n",
       "      <th>packaging_preference_Premium</th>\n",
       "      <th>packaging_preference_Simple</th>\n",
       "      <th>typical_consumption_situations_Casual (eg. At home)</th>\n",
       "      <th>typical_consumption_situations_Social (eg. Parties)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25400</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       income_levels  consume_frequency(weekly)  preferable_consumption_size  \\\n",
       "10996              3                          2                            2   \n",
       "25400              1                          0                            1   \n",
       "\n",
       "       health_concerns  age_group  cf_ab_score  zas_score  bsi  gender_M  \\\n",
       "10996                0          1         0.50          3    1         1   \n",
       "25400                2          3         0.25         12    0         1   \n",
       "\n",
       "       zone_rural  ...  awareness_of_other_brands_above 4  \\\n",
       "10996           0  ...                                  1   \n",
       "25400           0  ...                                  1   \n",
       "\n",
       "       reasons_for_choosing_brands_brand reputation  \\\n",
       "10996                                             0   \n",
       "25400                                             0   \n",
       "\n",
       "       reasons_for_choosing_brands_price  reasons_for_choosing_brands_quality  \\\n",
       "10996                                  1                                    0   \n",
       "25400                                  0                                    0   \n",
       "\n",
       "       flavor_preference_Traditional  purchase_channel_Retail Store  \\\n",
       "10996                              1                              0   \n",
       "25400                              0                              1   \n",
       "\n",
       "       packaging_preference_Premium  packaging_preference_Simple  \\\n",
       "10996                             0                            1   \n",
       "25400                             0                            0   \n",
       "\n",
       "       typical_consumption_situations_Casual (eg. At home)  \\\n",
       "10996                                                  0     \n",
       "25400                                                  0     \n",
       "\n",
       "       typical_consumption_situations_Social (eg. Parties)  \n",
       "10996                                                  1    \n",
       "25400                                                  1    \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "542bccc9-82b0-431a-bfd6-c872c9f24286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income_levels</th>\n",
       "      <th>consume_frequency(weekly)</th>\n",
       "      <th>preferable_consumption_size</th>\n",
       "      <th>health_concerns</th>\n",
       "      <th>age_group</th>\n",
       "      <th>cf_ab_score</th>\n",
       "      <th>zas_score</th>\n",
       "      <th>bsi</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>zone_rural</th>\n",
       "      <th>...</th>\n",
       "      <th>awareness_of_other_brands_above 4</th>\n",
       "      <th>reasons_for_choosing_brands_brand reputation</th>\n",
       "      <th>reasons_for_choosing_brands_price</th>\n",
       "      <th>reasons_for_choosing_brands_quality</th>\n",
       "      <th>flavor_preference_Traditional</th>\n",
       "      <th>purchase_channel_Retail Store</th>\n",
       "      <th>packaging_preference_Premium</th>\n",
       "      <th>packaging_preference_Simple</th>\n",
       "      <th>typical_consumption_situations_Casual (eg. At home)</th>\n",
       "      <th>typical_consumption_situations_Social (eg. Parties)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28430</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       income_levels  consume_frequency(weekly)  preferable_consumption_size  \\\n",
       "28430              5                          0                            0   \n",
       "3610               3                          1                            2   \n",
       "\n",
       "       health_concerns  age_group  cf_ab_score  zas_score  bsi  gender_M  \\\n",
       "28430                0          0         0.50          0    0         0   \n",
       "3610                 1          1         0.67          2    1         0   \n",
       "\n",
       "       zone_rural  ...  awareness_of_other_brands_above 4  \\\n",
       "28430           0  ...                                  0   \n",
       "3610            0  ...                                  0   \n",
       "\n",
       "       reasons_for_choosing_brands_brand reputation  \\\n",
       "28430                                             0   \n",
       "3610                                              0   \n",
       "\n",
       "       reasons_for_choosing_brands_price  reasons_for_choosing_brands_quality  \\\n",
       "28430                                  1                                    0   \n",
       "3610                                   1                                    0   \n",
       "\n",
       "       flavor_preference_Traditional  purchase_channel_Retail Store  \\\n",
       "28430                              1                              0   \n",
       "3610                               1                              0   \n",
       "\n",
       "       packaging_preference_Premium  packaging_preference_Simple  \\\n",
       "28430                             0                            0   \n",
       "3610                              1                            0   \n",
       "\n",
       "       typical_consumption_situations_Casual (eg. At home)  \\\n",
       "28430                                                  1     \n",
       "3610                                                   1     \n",
       "\n",
       "       typical_consumption_situations_Social (eg. Parties)  \n",
       "28430                                                  0    \n",
       "3610                                                   0    \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3d2df9c-8bcc-4d5d-8f38-5d5c6c0362c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22467,), (7489,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "256d96af-256d-48fc-80df-c0236c8911bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, ..., 0, 2, 3], shape=(22467,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5390ec85-d703-4893-8443-07969022b8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, ..., 0, 0, 0], shape=(7489,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2b94b27-7589-47ae-aaee-318374ef7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the X_train and X_test\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01ca6ed2-2c76-49ec-9d87-88414e768daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24063502,  1.23355365,  1.22938867, ...,  1.0401057 ,\n",
       "        -0.70866528,  1.56781937],\n",
       "       [-0.83579048, -1.34164146, -0.11664814, ..., -0.96144075,\n",
       "        -0.70866528,  1.56781937],\n",
       "       [-0.83579048, -1.34164146,  1.22938867, ..., -0.96144075,\n",
       "         1.41110343, -0.63782858],\n",
       "       ...,\n",
       "       [ 0.24063502, -0.05404391, -1.46268494, ..., -0.96144075,\n",
       "        -0.70866528, -0.63782858],\n",
       "       [-0.83579048,  1.23355365,  1.22938867, ..., -0.96144075,\n",
       "         1.41110343, -0.63782858],\n",
       "       [ 1.31706053, -1.34164146,  1.22938867, ...,  1.0401057 ,\n",
       "        -0.70866528, -0.63782858]], shape=(22467, 27))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd1ab775-8d9f-4240-a15b-89b523383efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.31706053, -1.34164146, -1.46268494, ..., -0.96144075,\n",
       "         1.41110343, -0.63782858],\n",
       "       [ 0.24063502, -0.05404391,  1.22938867, ..., -0.96144075,\n",
       "         1.41110343, -0.63782858],\n",
       "       [ 0.77884777,  1.23355365, -1.46268494, ...,  1.0401057 ,\n",
       "        -0.70866528, -0.63782858],\n",
       "       ...,\n",
       "       [ 1.31706053, -0.05404391, -1.46268494, ...,  1.0401057 ,\n",
       "        -0.70866528,  1.56781937],\n",
       "       [ 0.24063502, -1.34164146, -0.11664814, ..., -0.96144075,\n",
       "         1.41110343, -0.63782858],\n",
       "       [-1.37400323, -0.05404391, -0.11664814, ...,  1.0401057 ,\n",
       "        -0.70866528, -0.63782858]], shape=(7489, 27))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "446a6c91-3b3d-4fc0-a132-868f70a5bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8050474028575244\n",
      "\n",
      "Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76      1948\n",
      "           1       0.75      0.76      0.76      2199\n",
      "           2       0.90      0.89      0.90      2428\n",
      "           3       0.80      0.75      0.78       914\n",
      "\n",
      "    accuracy                           0.81      7489\n",
      "   macro avg       0.80      0.79      0.80      7489\n",
      "weighted avg       0.81      0.81      0.81      7489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"\\nClassification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "185b80bc-8a9c-4b3d-99f0-b388b1a2d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.559887835492055\n",
      "\n",
      "Classification Report (Naive Bayes):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.30      0.37      1948\n",
      "           1       0.54      0.29      0.38      2199\n",
      "           2       0.66      0.89      0.76      2428\n",
      "           3       0.44      0.89      0.59       914\n",
      "\n",
      "    accuracy                           0.56      7489\n",
      "   macro avg       0.53      0.59      0.52      7489\n",
      "weighted avg       0.55      0.56      0.52      7489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"\\nClassification Report (Naive Bayes):\")\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52e593c6-f4a9-43bb-b0fd-14aa9ec2f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8963813593270129\n",
      "\n",
      "Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      1948\n",
      "           1       0.84      0.89      0.87      2199\n",
      "           2       0.94      0.93      0.93      2428\n",
      "           3       0.92      0.89      0.91       914\n",
      "\n",
      "    accuracy                           0.90      7489\n",
      "   macro avg       0.90      0.89      0.90      7489\n",
      "weighted avg       0.90      0.90      0.90      7489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3874b531-e81e-4561-9acc-1137b79adec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8621978902390173\n",
      "\n",
      "Classification Report (SVM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84      1948\n",
      "           1       0.81      0.84      0.83      2199\n",
      "           2       0.92      0.91      0.92      2428\n",
      "           3       0.88      0.82      0.85       914\n",
      "\n",
      "    accuracy                           0.86      7489\n",
      "   macro avg       0.86      0.85      0.86      7489\n",
      "weighted avg       0.86      0.86      0.86      7489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"\\nClassification Report (SVM):\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d62d7691-1d1c-4c13-bc24-9c4b70563848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:56:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9262918947790092\n",
      "\n",
      "Classification Report (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      1948\n",
      "           1       0.89      0.92      0.91      2199\n",
      "           2       0.96      0.95      0.96      2428\n",
      "           3       0.94      0.92      0.93       914\n",
      "\n",
      "    accuracy                           0.93      7489\n",
      "   macro avg       0.93      0.92      0.93      7489\n",
      "weighted avg       0.93      0.93      0.93      7489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"\\nClassification Report (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "664baa39-abaa-4133-ba00-39ea647c8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9241554279610095\n",
      "\n",
      "Classification Report (LightGBM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      1948\n",
      "           1       0.89      0.92      0.90      2199\n",
      "           2       0.96      0.95      0.95      2428\n",
      "           3       0.94      0.92      0.93       914\n",
      "\n",
      "    accuracy                           0.92      7489\n",
      "   macro avg       0.93      0.92      0.92      7489\n",
      "weighted avg       0.92      0.92      0.92      7489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "\n",
    "lgb.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred_lgb))\n",
    "print(\"\\nClassification Report (LightGBM):\")\n",
    "print(classification_report(y_test, y_pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f733a3e2-318f-4a26-8a14-2c5a5707d85d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 19:56:47,865] A new study created in memory with name: no-name-08a1c911-fa7b-47df-b9af-aa129b6f046b\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:48,360] Trial 0 finished with value: 0.6746695201789342 and parameters: {'C': 0.0006323594574937032, 'penalty': 'l2', 'solver': 'liblinear'}. Best is trial 0 with value: 0.6746695201789342.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:48,841] Trial 1 finished with value: 0.6856659271355593 and parameters: {'C': 0.0008517221977451158, 'penalty': 'l2', 'solver': 'liblinear'}. Best is trial 1 with value: 0.6856659271355593.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:50,100] Trial 2 finished with value: 0.752714953114883 and parameters: {'C': 0.07447313184156007, 'penalty': 'l2', 'solver': 'liblinear'}. Best is trial 2 with value: 0.752714953114883.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:50,485] Trial 3 finished with value: 0.7928320101367102 and parameters: {'C': 0.009015028112609152, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 3 with value: 0.7928320101367102.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:50,960] Trial 4 finished with value: 0.7989984475123133 and parameters: {'C': 0.017793448354456336, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 4 with value: 0.7989984475123133.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:51,089] Trial 5 finished with value: 0.5851221039538385 and parameters: {'C': 0.00010109169383934234, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 4 with value: 0.7989984475123133.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:52,060] Trial 6 finished with value: 0.8051901030881693 and parameters: {'C': 0.593559323600665, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 6 with value: 0.8051901030881693.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:53,729] Trial 7 finished with value: 0.7622513010099329 and parameters: {'C': 3.4966449709120866, 'penalty': 'l2', 'solver': 'liblinear'}. Best is trial 6 with value: 0.8051901030881693.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:54,074] Trial 8 finished with value: 0.7660276450300931 and parameters: {'C': 0.001825760982160762, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 6 with value: 0.8051901030881693.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:54,559] Trial 9 finished with value: 0.6818479747069361 and parameters: {'C': 0.0007582008294642269, 'penalty': 'l2', 'solver': 'liblinear'}. Best is trial 6 with value: 0.8051901030881693.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:55,479] Trial 10 finished with value: 0.8051992203955277 and parameters: {'C': 1.1283292098246234, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 10 with value: 0.8051992203955277.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:56,572] Trial 11 finished with value: 0.805211665489092 and parameters: {'C': 1.1666977676302712, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 11 with value: 0.805211665489092.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:57,421] Trial 12 finished with value: 0.8057048401113878 and parameters: {'C': 6.320627081465339, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 12 with value: 0.8057048401113878.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:58,575] Trial 13 finished with value: 0.8050286606401339 and parameters: {'C': 8.980547498088875, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 12 with value: 0.8057048401113878.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:56:59,275] Trial 14 finished with value: 0.8065448333928382 and parameters: {'C': 0.2208146888457196, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 14 with value: 0.8065448333928382.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:00,274] Trial 15 finished with value: 0.8064168953100136 and parameters: {'C': 0.15319365276378932, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 14 with value: 0.8065448333928382.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:01,396] Trial 16 finished with value: 0.8058692030593432 and parameters: {'C': 0.1195367615965847, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 14 with value: 0.8065448333928382.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:02,151] Trial 17 finished with value: 0.8066764736566305 and parameters: {'C': 0.21370375487058627, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 17 with value: 0.8066764736566305.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:02,997] Trial 18 finished with value: 0.8053192614042854 and parameters: {'C': 0.34086716739377343, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 17 with value: 0.8066764736566305.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:03,480] Trial 19 finished with value: 0.8022775251443182 and parameters: {'C': 0.026158545298010687, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 17 with value: 0.8066764736566305.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:03,861] Trial 20 finished with value: 0.7879249264220137 and parameters: {'C': 0.006655153418096539, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 17 with value: 0.8066764736566305.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:04,550] Trial 21 finished with value: 0.8062643417960791 and parameters: {'C': 0.1452955083208938, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 17 with value: 0.8066764736566305.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:05,297] Trial 22 finished with value: 0.8049436371856601 and parameters: {'C': 0.06817849366156012, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 17 with value: 0.8066764736566305.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:06,208] Trial 23 finished with value: 0.8066835939996075 and parameters: {'C': 0.228106838302619, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 23 with value: 0.8066835939996075.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:07,142] Trial 24 finished with value: 0.806280506780381 and parameters: {'C': 0.2659913561681861, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 23 with value: 0.8066835939996075.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:07,999] Trial 25 finished with value: 0.8054415262765486 and parameters: {'C': 2.3854006744607106, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 23 with value: 0.8066835939996075.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:09,083] Trial 26 finished with value: 0.7508951761860119 and parameters: {'C': 0.043277278561894214, 'penalty': 'l2', 'solver': 'liblinear'}. Best is trial 23 with value: 0.8066835939996075.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:09,973] Trial 27 finished with value: 0.8050714472497655 and parameters: {'C': 0.49922153789640433, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 23 with value: 0.8066835939996075.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:10,954] Trial 28 finished with value: 0.8050593392558848 and parameters: {'C': 0.6323939427677994, 'penalty': 'l2', 'solver': 'lbfgs'}. Best is trial 23 with value: 0.8066835939996075.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\708379287.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform('C', 1e-4, 10)\n",
      "[I 2025-11-21 19:57:11,961] Trial 29 finished with value: 0.7417552512029197 and parameters: {'C': 0.014106171315991569, 'penalty': 'l2', 'solver': 'liblinear'}. Best is trial 23 with value: 0.8066835939996075.\n"
     ]
    }
   ],
   "source": [
    "def objective_lr(trial):\n",
    "    C = trial.suggest_loguniform('C', 1e-4, 10)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l2\"])\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"lbfgs\", \"liblinear\"])\n",
    "    \n",
    "    model = LogisticRegression(\n",
    "        C=C,\n",
    "        penalty=penalty,\n",
    "        solver=solver,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "study_lr = optuna.create_study(direction='maximize')\n",
    "study_lr.optimize(objective_lr, n_trials=30)\n",
    "\n",
    "best_lr_params = study_lr.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "518e425a-339c-46fe-a69e-e76882964e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 19:57:11,979] A new study created in memory with name: no-name-71fa2a39-0303-4c99-ad78-fc5e8b0c6f9d\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,030] Trial 0 finished with value: 0.5406548598541123 and parameters: {'var_smoothing': 0.004833394917214636}. Best is trial 0 with value: 0.5406548598541123.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,081] Trial 1 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 2.4573958239598356e-07}. Best is trial 0 with value: 0.5406548598541123.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,140] Trial 2 finished with value: 0.5401546299600019 and parameters: {'var_smoothing': 0.004401056775989688}. Best is trial 0 with value: 0.5406548598541123.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,206] Trial 3 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 3.0219129385512804e-08}. Best is trial 0 with value: 0.5406548598541123.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,264] Trial 4 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 7.014288566870585e-06}. Best is trial 0 with value: 0.5406548598541123.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,320] Trial 5 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 2.6739158823735685e-11}. Best is trial 0 with value: 0.5406548598541123.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,374] Trial 6 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 2.0956751374723308e-10}. Best is trial 0 with value: 0.5406548598541123.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,426] Trial 7 finished with value: 0.5452299118706398 and parameters: {'var_smoothing': 0.008932371514411253}. Best is trial 7 with value: 0.5452299118706398.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,477] Trial 8 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 9.90308455783296e-09}. Best is trial 7 with value: 0.5452299118706398.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,527] Trial 9 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 1.855037719551732e-07}. Best is trial 7 with value: 0.5452299118706398.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,587] Trial 10 finished with value: 0.5248834822214211 and parameters: {'var_smoothing': 2.975089279572805e-05}. Best is trial 7 with value: 0.5452299118706398.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,647] Trial 11 finished with value: 0.5462862473955463 and parameters: {'var_smoothing': 0.00941087990531164}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,708] Trial 12 finished with value: 0.5250868151372394 and parameters: {'var_smoothing': 0.0002149981741825696}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,768] Trial 13 finished with value: 0.52562326171718 and parameters: {'var_smoothing': 0.0002897533961629246}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,827] Trial 14 finished with value: 0.5430803487426756 and parameters: {'var_smoothing': 0.007389712491495963}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,886] Trial 15 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 2.2656213537184383e-06}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:12,950] Trial 16 finished with value: 0.5250868151372394 and parameters: {'var_smoothing': 0.00021690419116448766}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,008] Trial 17 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 1.9974607143405292e-12}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,066] Trial 18 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 1.990077174847322e-09}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,125] Trial 19 finished with value: 0.5284723394937011 and parameters: {'var_smoothing': 0.0006202359317001001}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,185] Trial 20 finished with value: 0.5248023663099083 and parameters: {'var_smoothing': 2.5199777145788715e-05}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,244] Trial 21 finished with value: 0.5457583210051347 and parameters: {'var_smoothing': 0.009100951501912247}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,305] Trial 22 finished with value: 0.531071296697028 and parameters: {'var_smoothing': 0.0011951937154347305}. Best is trial 11 with value: 0.5462862473955463.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,365] Trial 23 finished with value: 0.5471534452496796 and parameters: {'var_smoothing': 0.00965827608456564}. Best is trial 23 with value: 0.5471534452496796.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,426] Trial 24 finished with value: 0.5247944255767353 and parameters: {'var_smoothing': 5.966955668599609e-05}. Best is trial 23 with value: 0.5471534452496796.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,488] Trial 25 finished with value: 0.5244214512906498 and parameters: {'var_smoothing': 1.299955232876559e-06}. Best is trial 23 with value: 0.5471534452496796.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,558] Trial 26 finished with value: 0.5303337235015043 and parameters: {'var_smoothing': 0.0009513757275844934}. Best is trial 23 with value: 0.5471534452496796.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,652] Trial 27 finished with value: 0.5340243504084525 and parameters: {'var_smoothing': 0.0020893073519120523}. Best is trial 23 with value: 0.5471534452496796.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,734] Trial 28 finished with value: 0.5251107345046215 and parameters: {'var_smoothing': 0.00016113935552267958}. Best is trial 23 with value: 0.5471534452496796.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2505787344.py:2: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
      "[I 2025-11-21 19:57:13,794] Trial 29 finished with value: 0.5478408935726977 and parameters: {'var_smoothing': 0.009900957034187585}. Best is trial 29 with value: 0.5478408935726977.\n"
     ]
    }
   ],
   "source": [
    "def objective_nb(trial):\n",
    "    var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-2)\n",
    "    \n",
    "    model = GaussianNB(var_smoothing=var_smoothing)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "study_nb = optuna.create_study(direction='maximize')\n",
    "study_nb.optimize(objective_nb, n_trials=30)\n",
    "\n",
    "best_nb_params = study_nb.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "779252d9-92e1-4bcd-beb8-e1dec1524d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 19:57:13,812] A new study created in memory with name: no-name-f43ed077-ab46-413c-a3ae-4380c3fdceb8\n",
      "[I 2025-11-21 19:57:22,298] Trial 0 finished with value: 0.8674918795053722 and parameters: {'n_estimators': 256, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 9, 'bootstrap': True}. Best is trial 0 with value: 0.8674918795053722.\n",
      "[I 2025-11-21 19:57:38,871] Trial 1 finished with value: 0.8503420339082973 and parameters: {'n_estimators': 451, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 0 with value: 0.8674918795053722.\n",
      "[I 2025-11-21 19:57:47,840] Trial 2 finished with value: 0.8874015790316977 and parameters: {'n_estimators': 258, 'max_depth': 50, 'min_samples_split': 13, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.8874015790316977.\n",
      "[I 2025-11-21 19:58:03,471] Trial 3 finished with value: 0.8989467473561913 and parameters: {'n_estimators': 315, 'max_depth': 20, 'min_samples_split': 11, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:58:17,737] Trial 4 finished with value: 0.8839152166132196 and parameters: {'n_estimators': 383, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:58:32,813] Trial 5 finished with value: 0.8832703213354267 and parameters: {'n_estimators': 334, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:58:38,408] Trial 6 finished with value: 0.8862824949981551 and parameters: {'n_estimators': 120, 'max_depth': 37, 'min_samples_split': 2, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:58:51,882] Trial 7 finished with value: 0.885864187070125 and parameters: {'n_estimators': 306, 'max_depth': 44, 'min_samples_split': 7, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:59:03,379] Trial 8 finished with value: 0.8836512265341492 and parameters: {'n_estimators': 329, 'max_depth': 34, 'min_samples_split': 3, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:59:18,884] Trial 9 finished with value: 0.8660027268627606 and parameters: {'n_estimators': 462, 'max_depth': 40, 'min_samples_split': 8, 'min_samples_leaf': 10, 'bootstrap': True}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:59:26,675] Trial 10 finished with value: 0.8919703061807884 and parameters: {'n_estimators': 174, 'max_depth': 20, 'min_samples_split': 20, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:59:33,080] Trial 11 finished with value: 0.8902014811647375 and parameters: {'n_estimators': 147, 'max_depth': 20, 'min_samples_split': 20, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:59:41,784] Trial 12 finished with value: 0.893256849599003 and parameters: {'n_estimators': 190, 'max_depth': 18, 'min_samples_split': 15, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:59:46,264] Trial 13 finished with value: 0.7191259881878647 and parameters: {'n_estimators': 223, 'max_depth': 5, 'min_samples_split': 15, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 19:59:55,896] Trial 14 finished with value: 0.889972171974221 and parameters: {'n_estimators': 211, 'max_depth': 15, 'min_samples_split': 16, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:00:12,352] Trial 15 finished with value: 0.8936394888876877 and parameters: {'n_estimators': 350, 'max_depth': 26, 'min_samples_split': 13, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:00:31,533] Trial 16 finished with value: 0.8979907762077647 and parameters: {'n_estimators': 398, 'max_depth': 26, 'min_samples_split': 11, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:00:50,971] Trial 17 finished with value: 0.8954868740469686 and parameters: {'n_estimators': 408, 'max_depth': 29, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:01:14,693] Trial 18 finished with value: 0.8926121424758608 and parameters: {'n_estimators': 496, 'max_depth': 23, 'min_samples_split': 12, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:01:32,438] Trial 19 finished with value: 0.8858651317184032 and parameters: {'n_estimators': 393, 'max_depth': 34, 'min_samples_split': 17, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:01:43,634] Trial 20 finished with value: 0.7490416670535248 and parameters: {'n_estimators': 423, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:02:01,992] Trial 21 finished with value: 0.8953696473636675 and parameters: {'n_estimators': 375, 'max_depth': 30, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:02:22,345] Trial 22 finished with value: 0.8956638875012964 and parameters: {'n_estimators': 412, 'max_depth': 28, 'min_samples_split': 9, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:02:36,195] Trial 23 finished with value: 0.8924638705941492 and parameters: {'n_estimators': 283, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:02:56,153] Trial 24 finished with value: 0.8876589415092776 and parameters: {'n_estimators': 439, 'max_depth': 16, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 3 with value: 0.8989467473561913.\n",
      "[I 2025-11-21 20:03:14,427] Trial 25 finished with value: 0.9004140834790466 and parameters: {'n_estimators': 359, 'max_depth': 27, 'min_samples_split': 8, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 25 with value: 0.9004140834790466.\n",
      "[I 2025-11-21 20:03:28,414] Trial 26 finished with value: 0.8937905811809164 and parameters: {'n_estimators': 360, 'max_depth': 33, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 25 with value: 0.9004140834790466.\n",
      "[I 2025-11-21 20:03:43,189] Trial 27 finished with value: 0.8955981336658818 and parameters: {'n_estimators': 313, 'max_depth': 21, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 25 with value: 0.9004140834790466.\n",
      "[I 2025-11-21 20:03:56,598] Trial 28 finished with value: 0.8969721648418074 and parameters: {'n_estimators': 282, 'max_depth': 26, 'min_samples_split': 13, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 25 with value: 0.9004140834790466.\n",
      "[I 2025-11-21 20:04:11,675] Trial 29 finished with value: 0.8598781937103456 and parameters: {'n_estimators': 500, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 25 with value: 0.9004140834790466.\n",
      "[I 2025-11-21 20:04:24,743] Trial 30 finished with value: 0.9009496511050383 and parameters: {'n_estimators': 252, 'max_depth': 32, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 30 with value: 0.9009496511050383.\n",
      "[I 2025-11-21 20:04:38,416] Trial 31 finished with value: 0.9004379815520848 and parameters: {'n_estimators': 253, 'max_depth': 32, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 30 with value: 0.9009496511050383.\n",
      "[I 2025-11-21 20:04:51,898] Trial 32 finished with value: 0.9009734624581738 and parameters: {'n_estimators': 250, 'max_depth': 39, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 32 with value: 0.9009734624581738.\n",
      "[I 2025-11-21 20:05:05,404] Trial 33 finished with value: 0.9010966812725547 and parameters: {'n_estimators': 249, 'max_depth': 42, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 33 with value: 0.9010966812725547.\n",
      "[I 2025-11-21 20:05:18,343] Trial 34 finished with value: 0.9007004974224145 and parameters: {'n_estimators': 240, 'max_depth': 44, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 33 with value: 0.9010966812725547.\n",
      "[I 2025-11-21 20:05:30,002] Trial 35 finished with value: 0.8923374466667027 and parameters: {'n_estimators': 242, 'max_depth': 47, 'min_samples_split': 6, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 33 with value: 0.9010966812725547.\n",
      "[I 2025-11-21 20:05:41,757] Trial 36 finished with value: 0.8934386751170695 and parameters: {'n_estimators': 276, 'max_depth': 40, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 33 with value: 0.9010966812725547.\n",
      "[I 2025-11-21 20:05:52,140] Trial 37 finished with value: 0.8924831085191696 and parameters: {'n_estimators': 212, 'max_depth': 43, 'min_samples_split': 3, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 33 with value: 0.9010966812725547.\n",
      "[I 2025-11-21 20:06:00,478] Trial 38 finished with value: 0.892877649243421 and parameters: {'n_estimators': 178, 'max_depth': 49, 'min_samples_split': 5, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 33 with value: 0.9010966812725547.\n",
      "[I 2025-11-21 20:06:11,097] Trial 39 finished with value: 0.8976282143197997 and parameters: {'n_estimators': 230, 'max_depth': 38, 'min_samples_split': 2, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 33 with value: 0.9010966812725547.\n"
     ]
    }
   ],
   "source": [
    "def objective_rf(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    bootstrap = trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(objective_rf, n_trials=40)\n",
    "\n",
    "best_rf_params = study_rf.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b31acb0b-6592-4646-bf3d-44307e6c49dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 20:06:11,138] A new study created in memory with name: no-name-029212df-8560-4153-ab7e-dc48f2dcc42f\n",
      "[I 2025-11-21 20:13:25,956] Trial 0 finished with value: 0.21326874899836495 and parameters: {'C': 0.18393239939519038, 'gamma': 0.0072901870591695955, 'kernel': 'poly'}. Best is trial 0 with value: 0.21326874899836495.\n",
      "[I 2025-11-21 20:18:44,385] Trial 1 finished with value: 0.8607751270311095 and parameters: {'C': 1.2427326080095387, 'gamma': 0.11565003896016061, 'kernel': 'rbf'}. Best is trial 1 with value: 0.8607751270311095.\n"
     ]
    }
   ],
   "source": [
    "TARGET_F1 = 0.84\n",
    "\n",
    "def objective_svm(trial):\n",
    "\n",
    "    C = trial.suggest_float(\"C\", 1e-3, 10, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 1e-4, 1, log=True)\n",
    "    kernel = trial.suggest_categorical(\"kernel\", [\"rbf\", \"poly\"])\n",
    "\n",
    "    model = SVC(\n",
    "        C=C,\n",
    "        gamma=gamma,\n",
    "        kernel=kernel,\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    score = f1_score(y_test, preds, average=\"weighted\")\n",
    "\n",
    "    if score >= TARGET_F1:\n",
    "        trial.study.stop()\n",
    "\n",
    "    return score\n",
    "\n",
    "study_svm = optuna.create_study(direction=\"maximize\")\n",
    "study_svm.optimize(objective_svm, n_trials=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "051893c1-714e-43c1-a6f0-a4524af6198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 20:18:44,451] A new study created in memory with name: no-name-f7c47591-2fb3-4cbc-9ca2-783108221de8\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:18:50,953] Trial 0 finished with value: 0.8531969265401868 and parameters: {'max_depth': 4, 'learning_rate': 0.032611189387702466, 'n_estimators': 243, 'subsample': 0.6234428186579081, 'colsample_bytree': 0.8457061504920453, 'gamma': 3.170993454765279}. Best is trial 0 with value: 0.8531969265401868.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:18:57,365] Trial 1 finished with value: 0.817157258071383 and parameters: {'max_depth': 6, 'learning_rate': 0.0050124635439829125, 'n_estimators': 192, 'subsample': 0.941188106607086, 'colsample_bytree': 0.6063166970151561, 'gamma': 0.4421150820656633}. Best is trial 0 with value: 0.8531969265401868.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:19:10,404] Trial 2 finished with value: 0.9134521778913622 and parameters: {'max_depth': 10, 'learning_rate': 0.03873913951365257, 'n_estimators': 358, 'subsample': 0.8913570376723344, 'colsample_bytree': 0.5090153498587059, 'gamma': 1.327621247121476}. Best is trial 2 with value: 0.9134521778913622.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:19:23,357] Trial 3 finished with value: 0.9159150938441537 and parameters: {'max_depth': 9, 'learning_rate': 0.03328802710995828, 'n_estimators': 272, 'subsample': 0.524940533574767, 'colsample_bytree': 0.7582526266158174, 'gamma': 0.3790363447963657}. Best is trial 3 with value: 0.9159150938441537.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:19:28,001] Trial 4 finished with value: 0.7972503718029389 and parameters: {'max_depth': 6, 'learning_rate': 0.004192858480515187, 'n_estimators': 102, 'subsample': 0.9860441127295565, 'colsample_bytree': 0.9235566562703301, 'gamma': 0.15311905395251346}. Best is trial 3 with value: 0.9159150938441537.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:19:32,389] Trial 5 finished with value: 0.8657106409779927 and parameters: {'max_depth': 7, 'learning_rate': 0.03402854224478784, 'n_estimators': 114, 'subsample': 0.6932367227553089, 'colsample_bytree': 0.5329769364309904, 'gamma': 2.5352780853005457}. Best is trial 3 with value: 0.9159150938441537.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:19:39,335] Trial 6 finished with value: 0.8620380480040116 and parameters: {'max_depth': 7, 'learning_rate': 0.019261806793343775, 'n_estimators': 197, 'subsample': 0.61861645260838, 'colsample_bytree': 0.5387650914705875, 'gamma': 2.8889302254358458}. Best is trial 3 with value: 0.9159150938441537.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:19:48,809] Trial 7 finished with value: 0.9062017872680537 and parameters: {'max_depth': 10, 'learning_rate': 0.048710114315354526, 'n_estimators': 301, 'subsample': 0.5139877742803228, 'colsample_bytree': 0.6279929655506822, 'gamma': 2.1524377120759137}. Best is trial 3 with value: 0.9159150938441537.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:19:53,190] Trial 8 finished with value: 0.9071227024028627 and parameters: {'max_depth': 9, 'learning_rate': 0.14557475483465207, 'n_estimators': 187, 'subsample': 0.956461471791502, 'colsample_bytree': 0.8227044751358266, 'gamma': 4.087476550811392}. Best is trial 3 with value: 0.9159150938441537.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:20:01,647] Trial 9 finished with value: 0.9109014952263149 and parameters: {'max_depth': 7, 'learning_rate': 0.07456363767662569, 'n_estimators': 317, 'subsample': 0.6793716154324322, 'colsample_bytree': 0.5971137147046015, 'gamma': 2.7270373919800632}. Best is trial 3 with value: 0.9159150938441537.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:20:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:20:10,879] Trial 10 finished with value: 0.6834254404630985 and parameters: {'max_depth': 3, 'learning_rate': 0.0010665327948998068, 'n_estimators': 399, 'subsample': 0.8168644414061002, 'colsample_bytree': 0.7300015208129101, 'gamma': 1.2825672599058362}. Best is trial 3 with value: 0.9159150938441537.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:20:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:20:18,499] Trial 11 finished with value: 0.9264136181986439 and parameters: {'max_depth': 10, 'learning_rate': 0.23821014257700943, 'n_estimators': 387, 'subsample': 0.8789258607830428, 'colsample_bytree': 0.7162502114897272, 'gamma': 1.343052072776392}. Best is trial 11 with value: 0.9264136181986439.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:20:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:20:25,202] Trial 12 finished with value: 0.927360250182602 and parameters: {'max_depth': 9, 'learning_rate': 0.1932366193828161, 'n_estimators': 285, 'subsample': 0.8201035882600586, 'colsample_bytree': 0.7296273965716039, 'gamma': 1.1468108718103847}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:20:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:20:32,920] Trial 13 finished with value: 0.9259762754305035 and parameters: {'max_depth': 9, 'learning_rate': 0.2620025292134865, 'n_estimators': 398, 'subsample': 0.822039905321836, 'colsample_bytree': 0.7129970137976079, 'gamma': 1.4071651991567753}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:20:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:20:39,928] Trial 14 finished with value: 0.9239165364989402 and parameters: {'max_depth': 8, 'learning_rate': 0.21542055926526335, 'n_estimators': 341, 'subsample': 0.8147185828687662, 'colsample_bytree': 0.6765567391013171, 'gamma': 1.7782697433606032}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:20:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:20:45,647] Trial 15 finished with value: 0.9072840250201927 and parameters: {'max_depth': 10, 'learning_rate': 0.11295000496444577, 'n_estimators': 243, 'subsample': 0.8940312329479764, 'colsample_bytree': 0.8081832966280627, 'gamma': 4.97822927336443}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:20:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:20:56,129] Trial 16 finished with value: 0.9241711077658534 and parameters: {'max_depth': 8, 'learning_rate': 0.10368987840720281, 'n_estimators': 352, 'subsample': 0.7750908367364532, 'colsample_bytree': 0.9831937497429494, 'gamma': 0.6140872530870576}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:20:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:21:02,514] Trial 17 finished with value: 0.9269566731906512 and parameters: {'max_depth': 8, 'learning_rate': 0.2830511531049159, 'n_estimators': 276, 'subsample': 0.8912515138868491, 'colsample_bytree': 0.6644753275051145, 'gamma': 0.9287674128338252}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:21:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:21:11,955] Trial 18 finished with value: 0.817756832341523 and parameters: {'max_depth': 5, 'learning_rate': 0.008309516675949952, 'n_estimators': 279, 'subsample': 0.732917678748844, 'colsample_bytree': 0.6785868435027956, 'gamma': 0.8789734623549863}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:21:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:21:19,481] Trial 19 finished with value: 0.9161771353696088 and parameters: {'max_depth': 8, 'learning_rate': 0.0675866901991146, 'n_estimators': 218, 'subsample': 0.8805201548760434, 'colsample_bytree': 0.8848746017169022, 'gamma': 2.018996022236624}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:21:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:21:25,754] Trial 20 finished with value: 0.8762583077527037 and parameters: {'max_depth': 8, 'learning_rate': 0.01203683234738021, 'n_estimators': 144, 'subsample': 0.7642576135276482, 'colsample_bytree': 0.7817701723764077, 'gamma': 0.7930451319700194}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:21:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:21:32,241] Trial 21 finished with value: 0.9255390431239778 and parameters: {'max_depth': 9, 'learning_rate': 0.28036428323431584, 'n_estimators': 320, 'subsample': 0.8527683902906172, 'colsample_bytree': 0.6621781921427496, 'gamma': 1.2537066012404605}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:21:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:21:48,381] Trial 22 finished with value: 0.9148878711066821 and parameters: {'max_depth': 10, 'learning_rate': 0.17008662702566898, 'n_estimators': 275, 'subsample': 0.9214517142417045, 'colsample_bytree': 0.7388134815671035, 'gamma': 0.0214394649775822}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:21:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:21:55,311] Trial 23 finished with value: 0.9228394284726419 and parameters: {'max_depth': 9, 'learning_rate': 0.2807177014117137, 'n_estimators': 379, 'subsample': 0.8571170869576814, 'colsample_bytree': 0.6991715275453243, 'gamma': 1.819481991513533}. Best is trial 12 with value: 0.927360250182602.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:21:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:22:04,574] Trial 24 finished with value: 0.9277629871759127 and parameters: {'max_depth': 8, 'learning_rate': 0.13643985407278547, 'n_estimators': 302, 'subsample': 0.9324692954897728, 'colsample_bytree': 0.6339208268182275, 'gamma': 0.9684267781354762}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:22:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:22:13,221] Trial 25 finished with value: 0.9264176900262312 and parameters: {'max_depth': 8, 'learning_rate': 0.10733389689307839, 'n_estimators': 297, 'subsample': 0.9639366467437921, 'colsample_bytree': 0.6298842535210913, 'gamma': 0.9948465416813039}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:22:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:22:19,757] Trial 26 finished with value: 0.9252423442336554 and parameters: {'max_depth': 7, 'learning_rate': 0.15462847270568014, 'n_estimators': 259, 'subsample': 0.9252941634728611, 'colsample_bytree': 0.6531229510113769, 'gamma': 1.7253348668285406}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:22:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:22:26,216] Trial 27 finished with value: 0.9069216489002173 and parameters: {'max_depth': 6, 'learning_rate': 0.07328433309219197, 'n_estimators': 229, 'subsample': 0.9853645521057144, 'colsample_bytree': 0.5790538521002802, 'gamma': 3.3215580776838247}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:22:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:22:33,270] Trial 28 finished with value: 0.9177255127847 and parameters: {'max_depth': 8, 'learning_rate': 0.16374131108175807, 'n_estimators': 319, 'subsample': 0.7982881509418042, 'colsample_bytree': 0.5696404124242214, 'gamma': 2.2462420050900858}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:22:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:22:42,137] Trial 29 finished with value: 0.8666276519090004 and parameters: {'max_depth': 5, 'learning_rate': 0.022883710768798946, 'n_estimators': 292, 'subsample': 0.7206550929238204, 'colsample_bytree': 0.6248145470410629, 'gamma': 0.9712451212775126}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:22:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:22:49,663] Trial 30 finished with value: 0.9075835234931194 and parameters: {'max_depth': 9, 'learning_rate': 0.058235777272525484, 'n_estimators': 249, 'subsample': 0.8443099674602712, 'colsample_bytree': 0.7774628241548838, 'gamma': 3.59101053494824}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:22:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:22:58,139] Trial 31 finished with value: 0.9269933799116651 and parameters: {'max_depth': 8, 'learning_rate': 0.11203740638139252, 'n_estimators': 300, 'subsample': 0.9563611778124437, 'colsample_bytree': 0.6394735752289099, 'gamma': 0.9946970447543655}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:22:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:23:09,466] Trial 32 finished with value: 0.9275126307854688 and parameters: {'max_depth': 7, 'learning_rate': 0.0969129886239216, 'n_estimators': 338, 'subsample': 0.923196178652235, 'colsample_bytree': 0.6878847348382415, 'gamma': 0.5789328429151752}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:23:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:23:20,403] Trial 33 finished with value: 0.9260412635089024 and parameters: {'max_depth': 7, 'learning_rate': 0.0929442913140148, 'n_estimators': 336, 'subsample': 0.92912473557743, 'colsample_bytree': 0.7000057702883736, 'gamma': 0.4890258353533917}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:23:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:23:33,318] Trial 34 finished with value: 0.9196009194616989 and parameters: {'max_depth': 6, 'learning_rate': 0.04414248363765426, 'n_estimators': 368, 'subsample': 0.9578391667807211, 'colsample_bytree': 0.7593239893830164, 'gamma': 0.3874480647270219}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:23:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:23:41,182] Trial 35 finished with value: 0.9223188957587981 and parameters: {'max_depth': 7, 'learning_rate': 0.1253154964088773, 'n_estimators': 329, 'subsample': 0.989663532958214, 'colsample_bytree': 0.5633296377677396, 'gamma': 1.6180606745298864}. Best is trial 24 with value: 0.9277629871759127.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:23:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:23:50,593] Trial 36 finished with value: 0.928329934231703 and parameters: {'max_depth': 5, 'learning_rate': 0.19336237856087984, 'n_estimators': 305, 'subsample': 0.9156749953474919, 'colsample_bytree': 0.6005609384562036, 'gamma': 0.19700514760710697}. Best is trial 36 with value: 0.928329934231703.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:23:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:24:01,066] Trial 37 finished with value: 0.9289868953155066 and parameters: {'max_depth': 5, 'learning_rate': 0.18645060823508722, 'n_estimators': 354, 'subsample': 0.9132403456260648, 'colsample_bytree': 0.5074421169480205, 'gamma': 0.20203693931618974}. Best is trial 37 with value: 0.9289868953155066.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:24:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:24:12,183] Trial 38 finished with value: 0.7517476853408327 and parameters: {'max_depth': 5, 'learning_rate': 0.0011070602609458313, 'n_estimators': 359, 'subsample': 0.9162090338439275, 'colsample_bytree': 0.5115567606076699, 'gamma': 0.20305881935326658}. Best is trial 37 with value: 0.9289868953155066.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\3151079946.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:24:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-11-21 20:24:21,887] Trial 39 finished with value: 0.8635658323577062 and parameters: {'max_depth': 4, 'learning_rate': 0.027172136092400604, 'n_estimators': 350, 'subsample': 0.5579034057475033, 'colsample_bytree': 0.5398543431296411, 'gamma': 0.5875422808891031}. Best is trial 37 with value: 0.9289868953155066.\n"
     ]
    }
   ],
   "source": [
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 400),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"random_state\": 42,\n",
    "        \"use_label_encoder\": False\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=40)\n",
    "\n",
    "best_xgb_params = study_xgb.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "451c2a3d-78ca-480b-94c8-2143101fb492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-21 20:24:21,986] A new study created in memory with name: no-name-9713898f-db1c-4c47-900c-e57dff803e54\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:25:34,681] Trial 0 finished with value: 0.8831736229962631 and parameters: {'num_leaves': 266, 'learning_rate': 0.007921398698185738, 'n_estimators': 206, 'min_child_samples': 22, 'subsample': 0.893292648412735, 'colsample_bytree': 0.527047238979892}. Best is trial 0 with value: 0.8831736229962631.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:27:09,304] Trial 1 finished with value: 0.9172986774767008 and parameters: {'num_leaves': 228, 'learning_rate': 0.02670160394069609, 'n_estimators': 333, 'min_child_samples': 41, 'subsample': 0.8924672031269377, 'colsample_bytree': 0.8161561860500494}. Best is trial 1 with value: 0.9172986774767008.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:27:35,914] Trial 2 finished with value: 0.9149525043462698 and parameters: {'num_leaves': 112, 'learning_rate': 0.20604737531901435, 'n_estimators': 217, 'min_child_samples': 27, 'subsample': 0.7212970495394584, 'colsample_bytree': 0.905784008075473}. Best is trial 1 with value: 0.9172986774767008.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:27:55,525] Trial 3 finished with value: 0.8697153894941796 and parameters: {'num_leaves': 36, 'learning_rate': 0.007308919059381716, 'n_estimators': 320, 'min_child_samples': 15, 'subsample': 0.8340673158293687, 'colsample_bytree': 0.916455620868631}. Best is trial 1 with value: 0.9172986774767008.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:29:17,675] Trial 4 finished with value: 0.9161434281019668 and parameters: {'num_leaves': 255, 'learning_rate': 0.04110421340483352, 'n_estimators': 266, 'min_child_samples': 18, 'subsample': 0.6969328003523636, 'colsample_bytree': 0.6116202613968935}. Best is trial 1 with value: 0.9172986774767008.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:29:55,389] Trial 5 finished with value: 0.8879658265864958 and parameters: {'num_leaves': 208, 'learning_rate': 0.00782677087416234, 'n_estimators': 162, 'min_child_samples': 26, 'subsample': 0.7389328572551976, 'colsample_bytree': 0.6584420506066798}. Best is trial 1 with value: 0.9172986774767008.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:30:04,352] Trial 6 finished with value: 0.9240289045993977 and parameters: {'num_leaves': 29, 'learning_rate': 0.20884456583526168, 'n_estimators': 172, 'min_child_samples': 5, 'subsample': 0.9327441782497559, 'colsample_bytree': 0.5880100273673998}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:30:42,699] Trial 7 finished with value: 0.9164764699751136 and parameters: {'num_leaves': 141, 'learning_rate': 0.16366384034091747, 'n_estimators': 239, 'min_child_samples': 26, 'subsample': 0.7795289756578998, 'colsample_bytree': 0.8630176361120482}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:32:11,530] Trial 8 finished with value: 0.9146938201823516 and parameters: {'num_leaves': 176, 'learning_rate': 0.13027231753204088, 'n_estimators': 391, 'min_child_samples': 46, 'subsample': 0.5219900723781452, 'colsample_bytree': 0.702252319482622}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:33:03,589] Trial 9 finished with value: 0.9144531273907471 and parameters: {'num_leaves': 109, 'learning_rate': 0.1383043714615218, 'n_estimators': 392, 'min_child_samples': 23, 'subsample': 0.8929983807910176, 'colsample_bytree': 0.9097349028523116}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:33:08,631] Trial 10 finished with value: 0.8917638178589465 and parameters: {'num_leaves': 24, 'learning_rate': 0.05664170149981499, 'n_estimators': 101, 'min_child_samples': 5, 'subsample': 0.9846271797764734, 'colsample_bytree': 0.5040888417372423}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:34:13,236] Trial 11 finished with value: 0.8495731747457773 and parameters: {'num_leaves': 217, 'learning_rate': 0.0013249180940523334, 'n_estimators': 284, 'min_child_samples': 42, 'subsample': 0.999182879480433, 'colsample_bytree': 0.7579154529201168}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:34:25,999] Trial 12 finished with value: 0.9091842125424197 and parameters: {'num_leaves': 61, 'learning_rate': 0.028593446410153392, 'n_estimators': 152, 'min_child_samples': 37, 'subsample': 0.9200772924465948, 'colsample_bytree': 0.7989735107732033}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:35:51,771] Trial 13 finished with value: 0.8639460489273908 and parameters: {'num_leaves': 220, 'learning_rate': 0.0019312270474523385, 'n_estimators': 333, 'min_child_samples': 37, 'subsample': 0.6240244387421441, 'colsample_bytree': 0.9972775211733325}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:37:17,633] Trial 14 finished with value: 0.9120409272393164 and parameters: {'num_leaves': 292, 'learning_rate': 0.29180991755217417, 'n_estimators': 340, 'min_child_samples': 5, 'subsample': 0.8428904568811375, 'colsample_bytree': 0.5886669337517266}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:37:36,558] Trial 15 finished with value: 0.923637995970168 and parameters: {'num_leaves': 85, 'learning_rate': 0.07204331905895257, 'n_estimators': 175, 'min_child_samples': 49, 'subsample': 0.9556900555822514, 'colsample_bytree': 0.7937605641195843}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:37:52,207] Trial 16 finished with value: 0.9221966383742837 and parameters: {'num_leaves': 73, 'learning_rate': 0.08431334748736483, 'n_estimators': 161, 'min_child_samples': 11, 'subsample': 0.9484157010381327, 'colsample_bytree': 0.7088726706934334}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:38:12,887] Trial 17 finished with value: 0.9211262350750894 and parameters: {'num_leaves': 83, 'learning_rate': 0.08976197926306845, 'n_estimators': 188, 'min_child_samples': 34, 'subsample': 0.8252762409291887, 'colsample_bytree': 0.5928991033278508}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:38:21,838] Trial 18 finished with value: 0.8609310611674622 and parameters: {'num_leaves': 50, 'learning_rate': 0.01563272927340902, 'n_estimators': 102, 'min_child_samples': 50, 'subsample': 0.953691153237303, 'colsample_bytree': 0.6867724164077378}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:38:39,363] Trial 19 finished with value: 0.9156398437285963 and parameters: {'num_leaves': 96, 'learning_rate': 0.2999143032307996, 'n_estimators': 135, 'min_child_samples': 32, 'subsample': 0.6471694115012565, 'colsample_bytree': 0.7780065934424577}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:39:17,553] Trial 20 finished with value: 0.9169026070758957 and parameters: {'num_leaves': 150, 'learning_rate': 0.05242345195959113, 'n_estimators': 181, 'min_child_samples': 13, 'subsample': 0.7892806850334715, 'colsample_bytree': 0.638167381546456}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:39:31,030] Trial 21 finished with value: 0.9228414688880515 and parameters: {'num_leaves': 61, 'learning_rate': 0.08440050566299442, 'n_estimators': 139, 'min_child_samples': 10, 'subsample': 0.9418728374412672, 'colsample_bytree': 0.7302795696977221}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:39:38,696] Trial 22 finished with value: 0.9222109217257631 and parameters: {'num_leaves': 24, 'learning_rate': 0.0876613788222837, 'n_estimators': 132, 'min_child_samples': 10, 'subsample': 0.9513481470424083, 'colsample_bytree': 0.8412281484381944}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:39:58,637] Trial 23 finished with value: 0.900651011702078 and parameters: {'num_leaves': 59, 'learning_rate': 0.014054990355195888, 'n_estimators': 226, 'min_child_samples': 9, 'subsample': 0.8613408421889028, 'colsample_bytree': 0.7451324667760324}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:40:19,524] Trial 24 finished with value: 0.9157255913423478 and parameters: {'num_leaves': 123, 'learning_rate': 0.07882833502891722, 'n_estimators': 129, 'min_child_samples': 18, 'subsample': 0.9327952600398427, 'colsample_bytree': 0.5649127353897012}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:40:39,068] Trial 25 finished with value: 0.9197993744023678 and parameters: {'num_leaves': 81, 'learning_rate': 0.03250855211546645, 'n_estimators': 180, 'min_child_samples': 7, 'subsample': 0.9904802952381577, 'colsample_bytree': 0.7328772750078882}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:40:54,944] Trial 26 finished with value: 0.9189686276143102 and parameters: {'num_leaves': 47, 'learning_rate': 0.21255539855156727, 'n_estimators': 200, 'min_child_samples': 16, 'subsample': 0.8865931531768191, 'colsample_bytree': 0.6649656358861525}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:41:28,499] Trial 27 finished with value: 0.9158678904708569 and parameters: {'num_leaves': 184, 'learning_rate': 0.11823664125932638, 'n_estimators': 148, 'min_child_samples': 21, 'subsample': 0.7828677509725381, 'colsample_bytree': 0.547163433340426}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:41:45,429] Trial 28 finished with value: 0.8465988164698015 and parameters: {'num_leaves': 94, 'learning_rate': 0.003524865359125336, 'n_estimators': 123, 'min_child_samples': 32, 'subsample': 0.9624002933114363, 'colsample_bytree': 0.8657538975933934}. Best is trial 6 with value: 0.9240289045993977.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:41:58,161] Trial 29 finished with value: 0.9248159440428241 and parameters: {'num_leaves': 40, 'learning_rate': 0.056517786176267404, 'n_estimators': 201, 'min_child_samples': 50, 'subsample': 0.8644024474261028, 'colsample_bytree': 0.9720436066216198}. Best is trial 29 with value: 0.9248159440428241.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:42:12,233] Trial 30 finished with value: 0.8934900782742097 and parameters: {'num_leaves': 21, 'learning_rate': 0.020780340450734215, 'n_estimators': 255, 'min_child_samples': 48, 'subsample': 0.8693389284146885, 'colsample_bytree': 0.9836921797511889}. Best is trial 29 with value: 0.9248159440428241.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:42:33,544] Trial 31 finished with value: 0.9232560644177027 and parameters: {'num_leaves': 67, 'learning_rate': 0.05549215794996258, 'n_estimators': 204, 'min_child_samples': 45, 'subsample': 0.9240201682468208, 'colsample_bytree': 0.9545710702059693}. Best is trial 29 with value: 0.9248159440428241.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:42:46,834] Trial 32 finished with value: 0.9232169900685617 and parameters: {'num_leaves': 43, 'learning_rate': 0.044736260056675484, 'n_estimators': 202, 'min_child_samples': 45, 'subsample': 0.9098990075487685, 'colsample_bytree': 0.9736559207288191}. Best is trial 29 with value: 0.9248159440428241.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:43:19,780] Trial 33 finished with value: 0.9188426799072573 and parameters: {'num_leaves': 128, 'learning_rate': 0.0598797778095603, 'n_estimators': 223, 'min_child_samples': 43, 'subsample': 0.9153976537552687, 'colsample_bytree': 0.9478729075438214}. Best is trial 29 with value: 0.9248159440428241.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:43:38,669] Trial 34 finished with value: 0.9141290828936033 and parameters: {'num_leaves': 71, 'learning_rate': 0.021903867807744833, 'n_estimators': 176, 'min_child_samples': 50, 'subsample': 0.8226503117875772, 'colsample_bytree': 0.9426133261575804}. Best is trial 29 with value: 0.9248159440428241.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:43:50,300] Trial 35 finished with value: 0.9218760337542782 and parameters: {'num_leaves': 35, 'learning_rate': 0.18449275142412885, 'n_estimators': 215, 'min_child_samples': 39, 'subsample': 0.8793310022253447, 'colsample_bytree': 0.8880631433414683}. Best is trial 29 with value: 0.9248159440428241.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:44:24,274] Trial 36 finished with value: 0.9082999827715729 and parameters: {'num_leaves': 101, 'learning_rate': 0.01194721884312824, 'n_estimators': 280, 'min_child_samples': 47, 'subsample': 0.8986489994211775, 'colsample_bytree': 0.8221013516843464}. Best is trial 29 with value: 0.9248159440428241.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:44:38,527] Trial 37 finished with value: 0.9249723372168902 and parameters: {'num_leaves': 35, 'learning_rate': 0.04073062905508371, 'n_estimators': 236, 'min_child_samples': 44, 'subsample': 0.9686419293049904, 'colsample_bytree': 0.9414701274247627}. Best is trial 37 with value: 0.9249723372168902.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:45:02,249] Trial 38 finished with value: 0.9232402467679376 and parameters: {'num_leaves': 46, 'learning_rate': 0.03513246188112681, 'n_estimators': 248, 'min_child_samples': 50, 'subsample': 0.9704712970633087, 'colsample_bytree': 0.9183366947179665}. Best is trial 37 with value: 0.9249723372168902.\n",
      "C:\\Users\\hruth\\AppData\\Local\\Temp\\ipykernel_3404\\2745667369.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 108\n",
      "[LightGBM] [Info] Number of data points in the train set: 22467, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score -1.346461\n",
      "[LightGBM] [Info] Start training from score -1.225281\n",
      "[LightGBM] [Info] Start training from score -1.126505\n",
      "[LightGBM] [Info] Start training from score -2.103725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hruth\\anaconda3\\envs\\pytorch_envs\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-11-21 20:45:15,892] Trial 39 finished with value: 0.9249172646852244 and parameters: {'num_leaves': 35, 'learning_rate': 0.11095892303819017, 'n_estimators': 232, 'min_child_samples': 43, 'subsample': 0.5161543632703413, 'colsample_bytree': 0.8816586187959625}. Best is trial 37 with value: 0.9249723372168902.\n"
     ]
    }
   ],
   "source": [
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 400),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "\n",
    "    model = LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "study_lgb = optuna.create_study(direction='maximize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=40)\n",
    "\n",
    "best_lgb_params = study_lgb.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d2f92ec-1521-4971-af88-7487794c017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression {'C': 0.228106838302619, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      " GaussianNB {'var_smoothing': 0.009900957034187585}\n",
      "\n",
      " Random_Forest {'n_estimators': 249, 'max_depth': 42, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': False}\n",
      "\n",
      " SVM {'C': 1.2427326080095387, 'gamma': 0.11565003896016061, 'kernel': 'rbf'}\n",
      "\n",
      " XGB {'max_depth': 5, 'learning_rate': 0.18645060823508722, 'n_estimators': 354, 'subsample': 0.9132403456260648, 'colsample_bytree': 0.5074421169480205, 'gamma': 0.20203693931618974}\n",
      "\n",
      " LGBM {'num_leaves': 35, 'learning_rate': 0.04073062905508371, 'n_estimators': 236, 'min_child_samples': 44, 'subsample': 0.9686419293049904, 'colsample_bytree': 0.9414701274247627}\n"
     ]
    }
   ],
   "source": [
    "print(f\"logistic_regression {best_lr_params}\")\n",
    "print(f\"\\n GaussianNB {best_nb_params}\")\n",
    "print(f\"\\n Random_Forest {best_rf_params}\")\n",
    "print(f\"\\n SVM {study_svm.best_params}\")\n",
    "print(f\"\\n XGB {best_xgb_params}\")\n",
    "print(f\"\\n LGBM {best_lgb_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
